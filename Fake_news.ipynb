{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "import re\n",
    "import numpy as np\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "from nltk.tokenize import WhitespaceTokenizer\n",
    "import fasttext\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.utils import shuffle\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, Conv1D, GlobalMaxPooling1D, Dense, Dropout\n",
    "\n",
    "# Ensure necessary NLTK data is downloaded\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "# ----------------------------------------------\n",
    "# Data Loading and Initial Preprocessing\n",
    "# ----------------------------------------------\n",
    "\n",
    "# Read the true and fake news data files\n",
    "df_true = pd.read_csv('True.csv')\n",
    "df_fake = pd.read_csv('Fake.csv')\n",
    "\n",
    "# Create a new column 'value' and assign '1' for true news and '0' for fake news\n",
    "df_true['value'] = '1'\n",
    "df_fake['value'] = '0'\n",
    "\n",
    "# Concatenate the true and fake dataframes\n",
    "df = pd.concat([df_true, df_fake], axis=0)\n",
    "\n",
    "# ----------------------------------------------\n",
    "# Text Cleaning and Preprocessing\n",
    "# ----------------------------------------------\n",
    "\n",
    "# Function to clean text data\n",
    "def clean_text(text):\n",
    "    return re.sub(r'[^a-zA-Z\\s]', '', text)  # Remove non-alphabetic characters\n",
    "\n",
    "# Clean the 'title' column\n",
    "df['title'] = df['title'].apply(clean_text)\n",
    "\n",
    "# Tokenize the 'title' column\n",
    "tokenizer = WhitespaceTokenizer()\n",
    "df['title'] = df['title'].apply(tokenizer.tokenize)\n",
    "\n",
    "# Remove stopwords and perform lemmatization\n",
    "stopwords_list = set(stopwords.words('english'))\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def preprocess_text(tokens):\n",
    "    lowercase_tokens = [token.lower() for token in tokens if token.isalpha() and token.lower() not in stopwords_list]\n",
    "    lemmatized_tokens = [lemmatizer.lemmatize(token) for token in lowercase_tokens]\n",
    "    return lemmatized_tokens\n",
    "\n",
    "df['title'] = df['title'].apply(preprocess_text)\n",
    "\n",
    "# Save the preprocessed data to a new CSV file\n",
    "df.to_csv('preprocessed_news.csv', index=False)\n",
    "\n",
    "# ----------------------------------------------\n",
    "# FastText Vectorization\n",
    "# ----------------------------------------------\n",
    "\n",
    "# Prepare the training data file for FastText\n",
    "with open('fasttext_training_data.txt', 'w') as f:\n",
    "    for i, title in enumerate(df['title']):\n",
    "        label = '__label__' + str(df['value'].iloc[i])\n",
    "        title_str = ' '.join(str(token) for token in title)\n",
    "        f.write(label + ' ' + title_str + '\\n')\n",
    "\n",
    "# Train the FastText model\n",
    "fasttext_model = fasttext.train_supervised(input='fasttext_training_data.txt')\n",
    "\n",
    "# Load the pre-trained FastText word embeddings\n",
    "ft = fasttext.load_model('cc.en.300.bin')\n",
    "\n",
    "# Obtain the vector representation for each title\n",
    "title_vectors = []\n",
    "for title in df['title']:\n",
    "    embeddings = [ft.get_word_vector(str(word)) for word in title]\n",
    "    title_vector = np.mean(embeddings, axis=0)\n",
    "    title_vectors.append(title_vector)\n",
    "\n",
    "# Convert the list of title vectors to a NumPy array\n",
    "X = np.array(title_vectors)\n",
    "y = df['value'].values\n",
    "\n",
    "# ----------------------------------------------\n",
    "# Shuffle and Split Data\n",
    "# ----------------------------------------------\n",
    "\n",
    "# Shuffle the data\n",
    "X, y = shuffle(X, y, random_state=42)\n",
    "\n",
    "# Split the data into training, validation, and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.25, random_state=42)\n",
    "\n",
    "# Convert the target values to float\n",
    "y_train = y_train.astype(float)\n",
    "y_val = y_val.astype(float)\n",
    "y_test = y_test.astype(float)\n",
    "\n",
    "# ----------------------------------------------\n",
    "# Define and Compile the CNN Model\n",
    "# ----------------------------------------------\n",
    "\n",
    "# Define the CNN architecture\n",
    "model = Sequential()\n",
    "model.add(Embedding(input_dim=X.shape[0], output_dim=X.shape[1], input_length=X.shape[1]))\n",
    "model.add(Conv1D(filters=128, kernel_size=3, activation='relu'))\n",
    "model.add(GlobalMaxPooling1D())\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# ----------------------------------------------\n",
    "# Train the Model\n",
    "# ----------------------------------------------\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train, y_train, epochs=10, batch_size=32, validation_data=(X_val, y_val))\n",
    "\n",
    "# ----------------------------------------------\n",
    "# Evaluate the Model\n",
    "# ----------------------------------------------\n",
    "\n",
    "# Evaluate the model on the testing set\n",
    "loss, accuracy = model.evaluate(X_test, y_test)\n",
    "print(f'Test Loss: {loss:.4f}')\n",
    "print(f'Test Accuracy: {accuracy:.4f}')\n",
    "\n",
    "# Make predictions on the testing set\n",
    "y_pred_prob = model.predict(X_test)\n",
    "y_pred = (y_pred_prob > 0.5).astype(int).flatten()\n",
    "\n",
    "# Calculate the accuracy of the model\n",
    "test_accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f'Test Accuracy: {test_accuracy:.4f}')\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
